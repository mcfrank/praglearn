We see three main critiques raised by the reviewers:
 
1. Use of linguistic terminology was excessive or unclear.
 
These comments are extremely helpful for us in reaching the broad audience that we aim for in this work. If accepted we will be sure to take them into account when revising.
 
2. It would be more realistic to allow a richer model of literal meaning, allowing for multi-word phrases (Reviewer 5) and unbounded numbers of words and objects via non-parametric methods (Reviewer 3).
 
We agree completely with this comment. Both of these extensions are part of our larger research agenda (and unpublished work has investigated both). With respect to the current paper, however, we were attempting to strike a balance between clarity and sophistication. We see the heart of our work as the discussion of the contradictions between learning models and pragmatic reasoning models (sec. 2.1), and our proposal for resolving them (sec. 2.2). Right now the core literal learning part of our model (line 90) uses simple word/object associations, but in principle it would be straightforward to replace this component with any standard Bayesian language learning model (e.g. ref [25] or Xu & Tenenbaum 2007; see also our footnote 2). When integrated into the full model, though, even the current word/object associations end up producing rich and counter-intuitive behavior -- enough so that the bulk of the paper is taken up with explaining and demonstrating this behavior through simulations (sec. 3-5).
 
Using a more realistic model would add more complexity to be explained -- in fact, our current implementation does model multi-word utterances, but we disabled this for the current report because it didn't change any qualitative results. In addition, a more complex learning model would make it more difficult to isolate the effects of what we view as the primary contribution, the pragmatics/learning combination. For example, more realistic language learning models generally incorporate some sort of sparseness prior. We find that our model produces sparse lexicons (sec 5.1), and that it does so even when using our simple model which has no sparseness prior. This demonstrates that this bias is a (non-obvious) side-effect of the pragmatics/learning interplay. So we feel that establishing the behavior of this simple model is a necessary precondition to understanding or explaining the behavior of the more realistic models that will follow.
 
3. Limited number of simulations in the manuscript (Reviewer 5).
 
Again, we feel that there is a tradeoff between complexity and clarity in our current work. While it would be possible to run simulations with more words and objects, these simulations would go beyond the human experimental literature and might not provide insights into the behavior of the model with respect to the phenomena of interest. (We recognize that this may be a disciplinary issue, given that it is unusual in machine learning to see so much attention given to such small scale problems).
 
We would also like to clarify the simulation content of the current manuscript. Sections 3-5 present the results of six qualitatively different simulation studies. Figs. 1 & 2 present individual simulation runs for illustration, but population statistics for batch runs are given in Fig. 3. In future we plan to run human experiments to test some of the hypotheses generated by these simulations (e.g., the explanation of
disambiguation without mapping given in lines 292-305, and the discrepancy between Horn and specificity implicatures with regard to iterated learning, lines 393-404). We also plan to expand the model to handle multi-word sentences. Both of these moves will provide new datasets for future work.
 
Again, we appreciate the thoughtful reviews. Responses to some specific queries:
 
Reviewer 3: What we mean by a "novel" lexicon is that our agents start jointly using lexicons that previously, no agent was using. What we mean by "efficient" is that this lexicon selection process is systematically biased towards choosing "good" ones (e.g. in the Horn implicature case, Fig. 3 right-hand side). Ref [11], by comparison, considers how to use an existing lexicon efficiently, which is a different thing. (Our model reduces to the model of [11] in the case where the lexicon is shared between communicators and known a priori.) Thank you for the suggestion about Table 1, we will revise accordingly.
 
Reviewer 5: In all cases, we represent the lexicon as a matrix of numbers between 0 and 1, with each row constrained to sum to 1. In the Horn implicature cases, our prior is simply a uniform distribution over such matrices. In the specificity implicature case, we use a prior which encodes the knowledge that the word "all" refers to the ALL object but not the SOME-BUT-NOT-ALL object (a Dirichlet with pseudocounts favoring ALL), and place a uniform prior on the meanings of "some." Thank you for the suggestion of a heuristic comparison model--we will investigate this in our future work.
 
Reviewer 7: There is substantial empirical evidence that speakers adjust what words they're using based on feedback from their listeners (e.g. ref. [8]). On the other hand, it is unknown whether this behavior is optimal because ours is (to our knowledge) the first quantitative investigation of optimal behavior, so there hasn't been any way to check! Regarding the meaning of "some", there is a large body of research on children's quantifier use (reviewed in ref. [2]). To summarize: children begin by making systematic failures to reason that "some" implies SOME-BUT-NOT-ALL. (In the words of one author, they seem "more literal" than adults). Only around age five or older do they make this and other scalar implicatures correctly. So our simulations suggesting maintenance of the literal meaning (sec. 4.2) are at least prima facie consistent with the developmental literature.
